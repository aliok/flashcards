License
===========

Copyright [2011] [Ali Ok - aliok AT apache org]

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

   http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


Module
=============
This module generates the word sets. A Python project.

It goes through Wikipedia articles, follows links to Wikipedia articles until a certain point and determines the frequency of the words in an existing dictionary.
Thus, the most frequent words can be asked first.

The source of the output is the random ordered dictionary included in the sources : dictSet.py.

In the module _flashcards-data-service_, a dictionary of most frequent 15000 words are used, which is generated by this module.

The output is Python representation of the dictionary, because it will be used statically in the _flash-cards-data-service_ module.

Options/Operations
==========================

* crawl : Starts counting the words for articles. The starting point is German Wikipedia homepage. The script adds the links found on the
    pages to a queue, and then pops one article from the queue does the same thing again, until a certain number of articles are already examined.

    This operation just counts the words and saves them to a file along with the queue, and the set of visited articles (so that they won't be fetched twice
    when multiple executions happens with the same state file). That means, you can just do the crawling in an incremental way by crawling into an existing
    state file.

    This is the only operation that creates/modifies a state file. Other operations just use the state file to produce other things.

    An example:
    `./main.py --operation crawl --stateFile state.dat --numberOfArticles 10`

* printState : This is just for auditing purposes. Gives a verbose output(items in the queue and visited articles set) of a given state file.

    An example:
    `./main.py --operation printState --stateFile state.dat`


* printWordCounts : This is also for auditing purposes. Writes the word counts.

    An example:
    `./main.py --operation printWordCounts --stateFile state.dat > output.txt`


* sortWordSet : Sorts the word-article-translation list with respect to word-count map and outputs the result.

    Word-count map is read from a given state file.

    The result is separated into multiple sets of a certain size and output like that.

    An example:
    `./main.py --operation sortWordSet --stateFile state.dat --setSize 100 > sortedSet.txt`
